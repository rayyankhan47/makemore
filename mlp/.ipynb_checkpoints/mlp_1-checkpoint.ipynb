{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3382a3-7692-4495-a34e-dfa0c242c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2e07b1-4f12-48c0-87d1-21958595a7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543164df-74fb-4642-a2b1-96b7b200e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5b7235-6509-4577-baa3-791320c33d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers in alphabetical order\n",
    "chars = sorted(list(set(\"\".join(words)))) # because each letter of the alphabet is used at least once in the entire dataset, we just get the alphabet here.\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0f5e303d-546e-4861-8817-3d405fd831f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: take the n first chars to predict the (n+1)th char.\n",
    "X, Y = [], []\n",
    "for w in words[:5]: # lets just work with the first five words temporarily.\n",
    "\n",
    "    print(\"=======================\")\n",
    "    print(w)\n",
    "    context = [0] * block_size # padding\n",
    "    for ch in w + \".\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(\"\".join(itos[i] for i in context), \"--->\", itos[ix]) # this is just for us to understand the rolling window mechanism going on here.\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y) # much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45684c59-46ec-4c37-89d5-fcb506dd043b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now lets look at X and Y.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X, X\u001b[38;5;241m.\u001b[39mshape, Y, Y\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Now lets look at X and Y.\n",
    "print(X, X.shape, Y, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "eb33532b-a232-4b72-a8cc-7475e5168000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nEach 3-sized example we got from the first five words yielded this tensor X with\\na numerical 'translation' of each character in the example. And Y gives us what\\neach example is being mapped to.\\n\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Each 3-sized example we got from the first five words yielded this tensor X with\n",
    "a numerical 'translation' of each character in the example. And Y gives us what\n",
    "each example is being mapped to.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7732534c-3809-4e34-bc4f-6dc5b4dc75b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6767, -1.4600],\n",
       "         [ 2.1740, -1.8246],\n",
       "         [-0.7156,  0.5481],\n",
       "         [-0.7978,  0.9911],\n",
       "         [ 0.4465,  0.4863],\n",
       "         [ 0.4351,  0.6104],\n",
       "         [ 1.0215,  0.3487],\n",
       "         [ 1.0514, -0.4481],\n",
       "         [-0.5221, -0.4600],\n",
       "         [-1.0644, -0.4811],\n",
       "         [ 2.0537, -0.1411],\n",
       "         [ 1.1017,  0.2575],\n",
       "         [ 0.2567,  0.6146],\n",
       "         [-0.9069, -1.9906],\n",
       "         [ 0.0154,  1.9454],\n",
       "         [-1.6891,  0.3033],\n",
       "         [-0.2752, -0.0514],\n",
       "         [ 0.2357, -1.7690],\n",
       "         [-0.0562,  1.3208],\n",
       "         [ 0.3151, -0.2876],\n",
       "         [ 0.8195,  0.8923],\n",
       "         [-0.0743,  0.4049],\n",
       "         [-0.2207,  0.9640],\n",
       "         [ 0.0672,  0.6663],\n",
       "         [ 0.1731,  0.8541],\n",
       "         [-0.0519, -0.8815],\n",
       "         [-0.7921,  1.2556]]),\n",
       " torch.Size([27, 2]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))\n",
    "'''\n",
    "Now, we essentially have an embedding C: Alphabet -->Â R^2 that lets us take a char\n",
    "and place it in some space. With this space, we can see how certain characters\n",
    "might be \"closer\" to others in that space, which could indicate some sort of semantic\n",
    "\"closeness\". However, this may or may not be blackbox, i.e. we don't really know\n",
    "how or why certain characters might be closer to each other than others. I don't know,\n",
    "though, that's just what I think.\n",
    "'''\n",
    "C, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f0756c86-bb46-433a-baab-8b9c827d8098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNow, we could potentially just use C and index into it as a lookup table, i.e.\\nC[5] = tensor([0.1615, 1.3169]), and so the character \"e\", the fifth in the alphabet,\\nwould be embedded as that vector in R^2. However, there\\'s a more interesting way to\\ndo this which could change the way we consider a layer in our NN, and that is by\\nusing one-hot encoding.\\n'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now, we could potentially just use C and index into it as a lookup table, i.e.\n",
    "C[5] = tensor([0.1615, 1.3169]), and so the character \"e\", the fifth in the alphabet,\n",
    "would be embedded as that vector in R^2. However, there's a more interesting way to\n",
    "do this which could change the way we consider a layer in our NN, and that is by\n",
    "using one-hot encoding.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c014d335-5576-4d6d-aeef-3fa34df2f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C == C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "724231ee-90d3-49cd-b9cf-14cedac6e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effectively, because of how matrix multiplication works, these are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ec5e8f76-804a-4081-8a6f-f540ab6d019e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.4351,  0.6104]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.4351,  0.6104],\n",
       "          [-0.9069, -1.9906]],\n",
       " \n",
       "         [[ 0.4351,  0.6104],\n",
       "          [-0.9069, -1.9906],\n",
       "          [-0.9069, -1.9906]],\n",
       " \n",
       "         [[-0.9069, -1.9906],\n",
       "          [-0.9069, -1.9906],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [-1.6891,  0.3033]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [-1.6891,  0.3033],\n",
       "          [ 0.2567,  0.6146]],\n",
       " \n",
       "         [[-1.6891,  0.3033],\n",
       "          [ 0.2567,  0.6146],\n",
       "          [-1.0644, -0.4811]],\n",
       " \n",
       "         [[ 0.2567,  0.6146],\n",
       "          [-1.0644, -0.4811],\n",
       "          [-0.2207,  0.9640]],\n",
       " \n",
       "         [[-1.0644, -0.4811],\n",
       "          [-0.2207,  0.9640],\n",
       "          [-1.0644, -0.4811]],\n",
       " \n",
       "         [[-0.2207,  0.9640],\n",
       "          [-1.0644, -0.4811],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 2.1740, -1.8246],\n",
       "          [-0.2207,  0.9640]],\n",
       " \n",
       "         [[ 2.1740, -1.8246],\n",
       "          [-0.2207,  0.9640],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [-1.0644, -0.4811]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [-1.0644, -0.4811],\n",
       "          [ 0.3151, -0.2876]],\n",
       " \n",
       "         [[-1.0644, -0.4811],\n",
       "          [ 0.3151, -0.2876],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.3151, -0.2876],\n",
       "          [ 2.1740, -1.8246],\n",
       "          [-0.7156,  0.5481]],\n",
       " \n",
       "         [[ 2.1740, -1.8246],\n",
       "          [-0.7156,  0.5481],\n",
       "          [ 0.4351,  0.6104]],\n",
       " \n",
       "         [[-0.7156,  0.5481],\n",
       "          [ 0.4351,  0.6104],\n",
       "          [ 0.2567,  0.6146]],\n",
       " \n",
       "         [[ 0.4351,  0.6104],\n",
       "          [ 0.2567,  0.6146],\n",
       "          [ 0.2567,  0.6146]],\n",
       " \n",
       "         [[ 0.2567,  0.6146],\n",
       "          [ 0.2567,  0.6146],\n",
       "          [ 2.1740, -1.8246]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.6767, -1.4600],\n",
       "          [ 0.3151, -0.2876]],\n",
       " \n",
       "         [[ 0.6767, -1.4600],\n",
       "          [ 0.3151, -0.2876],\n",
       "          [-1.6891,  0.3033]],\n",
       " \n",
       "         [[ 0.3151, -0.2876],\n",
       "          [-1.6891,  0.3033],\n",
       "          [-0.2752, -0.0514]],\n",
       " \n",
       "         [[-1.6891,  0.3033],\n",
       "          [-0.2752, -0.0514],\n",
       "          [-0.5221, -0.4600]],\n",
       " \n",
       "         [[-0.2752, -0.0514],\n",
       "          [-0.5221, -0.4600],\n",
       "          [-1.0644, -0.4811]],\n",
       " \n",
       "         [[-0.5221, -0.4600],\n",
       "          [-1.0644, -0.4811],\n",
       "          [ 2.1740, -1.8246]]]),\n",
       " torch.Size([32, 3, 2]))"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In pytorch, you can index into tensors with tensors.\n",
    "C[X], C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "291ce5ae-e26f-4e69-b5e1-39257537b97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis essentially yielded a 3 dimensional tensor, where in the first dimension,\\nwe have 32 rows for each example, in the second dimension, we have 3 characters\\nfrom a given example, and in the third dimension, we have 2 numbers representing\\nthe \"location\" in R^2 of a given character. So, effectively, our embedding is C[x].\\n'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This essentially yielded a 3 dimensional tensor, where in the first dimension,\n",
    "we have 32 rows for each example, in the second dimension, we have 3 characters\n",
    "from a given example, and in the third dimension, we have 2 numbers representing\n",
    "the \"location\" in R^2 of a given character. So, effectively, our embedding is C[x].\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4ed717dd-1572-4f1b-b1bb-8eb38923afa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4fb12c42-6afa-43e1-9ba9-dec81f45ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the weights and biases of our first layer.\n",
    "W1 = torch.randn((6,100)) # 100 comes from the design choice to have 100 neurons in this layer.\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e6f72cb2-e6d3-451a-b987-754b4933a0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3738,  0.3254,  0.6489,  ..., -8.0834,  7.0093, -1.4243],\n",
       "         [ 0.1343,  2.8214, -0.8319,  ..., -6.1792,  4.1403, -1.7716],\n",
       "         [ 1.6309, -5.8741,  2.4456,  ..., -3.3116,  5.0700,  1.4534],\n",
       "         ...,\n",
       "         [-1.5120, -0.4351, -0.4935,  ..., -0.9953,  0.5370,  2.7139],\n",
       "         [ 0.4034, -1.9915, -0.2281,  ...,  0.1962, -0.2482,  3.7829],\n",
       "         [ 0.3638,  4.3168,  4.0955,  ..., -6.8554,  4.4774, -2.6569]]),\n",
       " torch.Size([32, 100]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's concatenate the dimension containing n from R^n to match the number of rows in W1, so that we can do embedding @ W1.\n",
    "h = emb.view(-1,6) @ W1 + b1 # -1 allows torch to infer the size. It is in this case emb.shape[0] == 32, we're just trying to avoid hardcoding, because we just happened to have 32 examples right now given we're only looking at 5 words.\n",
    "h, h.shape, h.dtype\n",
    "# btw, emb.view(-1,6) @ W1 gives a 32 x 100 matrix, and we are trying to add to it a vector with 100 elements. This will work because of pytorch broadcasting, and it will do it in a way that is what we are indeed looking for in terms of linear combinations.\n",
    "# 32, 100\n",
    "#  1, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5df95a40-c9eb-410d-97c5-da12ecdded17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3573,  0.3144,  0.5709,  ..., -1.0000,  1.0000, -0.8905],\n",
       "        [ 0.1335,  0.9929, -0.6815,  ..., -1.0000,  0.9995, -0.9438],\n",
       "        [ 0.9262, -1.0000,  0.9851,  ..., -0.9973,  0.9999,  0.8964],\n",
       "        ...,\n",
       "        [-0.9073, -0.4096, -0.4570,  ..., -0.7596,  0.4907,  0.9913],\n",
       "        [ 0.3829, -0.9634, -0.2242,  ...,  0.1937, -0.2432,  0.9990],\n",
       "        [ 0.3485,  0.9996,  0.9994,  ..., -1.0000,  0.9997, -0.9902]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also apply non-linearity in our layer before passing it on.\n",
    "h = torch.tanh(h)\n",
    "h # now we have nice values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2e8f8031-5ed8-4d56-9bcc-acebcdcbaabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "562c9fb5-aa1f-4403-948d-e296a0e40e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next layer:\n",
    "W2 = torch.randn((100, 27)) # input: 100 neurons. output: 27 neurons (last)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1970dd6f-a8cd-470e-bfac-1e6b58442e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2 # this should work because dimensionality matches up + broadcasting works correctly for biases\n",
    "# logits are what we're about to pass into softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1eb7a66c-c341-4ff5-9173-0c08302f7ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "909b2db7-40c0-4e44-81bc-b7db35a618c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2954e-07, 2.3288e-09, 2.4666e-09, 3.5603e-04, 3.7209e-10, 6.0136e-05,\n",
       "          3.5556e-04, 8.6100e-03, 1.0385e-10, 6.6911e-06, 8.9132e-07, 1.3650e-15,\n",
       "          2.8066e-02, 3.6939e-06, 7.0524e-07, 3.0179e-16, 2.9954e-12, 1.9045e-05,\n",
       "          6.8319e-07, 3.9826e-01, 5.7023e-19, 1.3491e-02, 4.5589e-08, 5.5076e-01,\n",
       "          8.9111e-09, 5.8631e-07, 9.0464e-06],\n",
       "         [4.3919e-10, 1.2281e-03, 1.8107e-11, 9.5388e-08, 1.4453e-11, 9.5511e-02,\n",
       "          1.3926e-09, 6.0043e-02, 2.2005e-12, 1.1213e-03, 4.1671e-04, 4.9161e-15,\n",
       "          2.8274e-03, 6.3692e-04, 9.2209e-03, 1.5515e-11, 1.6580e-16, 4.5126e-04,\n",
       "          2.6800e-10, 8.2281e-01, 6.8688e-12, 2.1659e-06, 4.4575e-10, 3.3080e-06,\n",
       "          1.5799e-11, 5.7303e-03, 2.5561e-06],\n",
       "         [3.1210e-08, 8.7654e-13, 1.3649e-13, 1.0025e-05, 5.7851e-17, 7.0417e-09,\n",
       "          5.9053e-07, 6.8777e-11, 1.3684e-07, 2.6362e-07, 1.6677e-06, 2.8271e-07,\n",
       "          1.5519e-07, 2.1449e-12, 2.0562e-12, 7.0600e-14, 6.0522e-12, 3.3815e-01,\n",
       "          6.6183e-01, 3.4456e-13, 2.8565e-16, 4.2701e-06, 2.8912e-06, 2.1132e-06,\n",
       "          4.1781e-10, 2.1482e-12, 3.7087e-09],\n",
       "         [1.6253e-07, 3.1427e-13, 3.0379e-12, 4.3667e-10, 1.3093e-09, 1.0585e-10,\n",
       "          4.7849e-13, 3.1942e-13, 2.9756e-10, 1.9618e-10, 5.1646e-11, 1.4814e-11,\n",
       "          2.8758e-06, 1.3149e-13, 3.0660e-16, 2.9362e-12, 7.8831e-12, 1.8635e-12,\n",
       "          6.2595e-09, 3.4526e-11, 4.0645e-15, 2.4516e-08, 7.2279e-09, 2.0245e-05,\n",
       "          9.9998e-01, 2.5974e-16, 5.9241e-12],\n",
       "         [1.8219e-05, 4.8054e-11, 2.5160e-08, 2.7756e-04, 4.5108e-05, 9.0211e-08,\n",
       "          9.9913e-01, 1.8011e-04, 7.2990e-06, 1.7065e-09, 3.7607e-16, 1.7592e-11,\n",
       "          1.0863e-12, 4.6880e-05, 8.5808e-11, 2.0027e-23, 3.5931e-18, 2.1059e-06,\n",
       "          1.3037e-17, 1.5748e-13, 6.7873e-17, 2.8038e-11, 7.8186e-13, 2.3196e-07,\n",
       "          9.5373e-14, 2.0706e-09, 2.8860e-04],\n",
       "         [2.2954e-07, 2.3288e-09, 2.4666e-09, 3.5603e-04, 3.7209e-10, 6.0136e-05,\n",
       "          3.5556e-04, 8.6100e-03, 1.0385e-10, 6.6911e-06, 8.9132e-07, 1.3650e-15,\n",
       "          2.8066e-02, 3.6939e-06, 7.0524e-07, 3.0179e-16, 2.9954e-12, 1.9045e-05,\n",
       "          6.8319e-07, 3.9826e-01, 5.7023e-19, 1.3491e-02, 4.5589e-08, 5.5076e-01,\n",
       "          8.9111e-09, 5.8631e-07, 9.0464e-06],\n",
       "         [1.5659e-14, 4.2850e-09, 6.1712e-20, 3.4999e-09, 9.7946e-14, 1.0519e-07,\n",
       "          5.4895e-13, 3.5579e-15, 3.7210e-13, 2.7555e-01, 4.3306e-03, 1.5525e-18,\n",
       "          6.2462e-04, 2.4695e-07, 7.1839e-01, 2.0401e-09, 8.4240e-16, 1.0095e-03,\n",
       "          5.1471e-06, 1.4409e-05, 2.7968e-16, 7.5998e-06, 3.7384e-12, 5.4148e-10,\n",
       "          6.3157e-05, 8.5183e-08, 1.3129e-08],\n",
       "         [5.2719e-03, 2.0358e-04, 3.8467e-03, 2.8943e-13, 4.5170e-10, 2.6126e-06,\n",
       "          9.6158e-02, 1.3398e-08, 1.8943e-04, 3.9474e-05, 8.6148e-10, 4.8146e-05,\n",
       "          1.1748e-10, 2.8865e-08, 4.1088e-10, 5.8638e-13, 9.3542e-13, 6.3731e-02,\n",
       "          3.8390e-10, 3.9537e-11, 2.6202e-02, 7.4279e-11, 3.0679e-06, 6.4475e-07,\n",
       "          2.1071e-15, 8.0430e-01, 5.4089e-10],\n",
       "         [8.5763e-12, 3.5684e-10, 7.5099e-16, 2.3022e-08, 1.6248e-12, 4.7082e-10,\n",
       "          4.6753e-11, 2.3283e-21, 6.5953e-08, 9.5039e-10, 9.6636e-13, 1.3270e-11,\n",
       "          1.3654e-11, 1.4777e-08, 3.5382e-13, 2.1320e-04, 3.8721e-16, 9.9824e-01,\n",
       "          2.6792e-14, 7.3866e-18, 1.2509e-18, 2.9201e-12, 1.7959e-12, 6.3634e-21,\n",
       "          1.5458e-03, 1.3780e-18, 6.0759e-11],\n",
       "         [2.9439e-05, 1.4033e-09, 9.8347e-01, 2.5770e-10, 6.2074e-08, 1.3921e-05,\n",
       "          1.7423e-10, 6.6270e-10, 7.3316e-09, 3.0083e-05, 6.0429e-06, 1.3198e-07,\n",
       "          2.7434e-04, 9.6601e-07, 1.4884e-13, 3.5963e-05, 1.1155e-02, 2.5281e-05,\n",
       "          1.4704e-10, 9.8661e-04, 6.0889e-05, 1.2854e-05, 4.3127e-04, 1.4186e-08,\n",
       "          5.7079e-06, 3.4593e-03, 1.7644e-06],\n",
       "         [3.2223e-08, 3.1386e-08, 2.3517e-13, 8.9223e-10, 4.8857e-12, 8.3177e-09,\n",
       "          7.2207e-09, 2.6690e-19, 2.9961e-04, 2.6222e-11, 2.5034e-10, 3.3170e-06,\n",
       "          6.1860e-13, 1.2277e-05, 1.5049e-12, 1.7128e-03, 7.7311e-16, 9.9797e-01,\n",
       "          4.7952e-13, 2.0706e-17, 4.5642e-16, 6.5017e-12, 4.4802e-10, 1.1299e-17,\n",
       "          1.5763e-06, 1.5506e-19, 5.0323e-11],\n",
       "         [4.0785e-06, 3.6177e-12, 9.9996e-01, 2.4361e-09, 7.0072e-10, 1.4441e-12,\n",
       "          1.9366e-06, 3.3422e-05, 4.9220e-07, 1.5988e-16, 2.4996e-21, 8.7817e-10,\n",
       "          3.5705e-14, 6.8895e-11, 2.3810e-20, 4.9387e-18, 1.2958e-08, 4.8626e-12,\n",
       "          2.1847e-15, 2.5487e-13, 1.3626e-14, 1.5192e-13, 6.0604e-10, 2.5498e-10,\n",
       "          9.1116e-13, 1.5796e-13, 7.6370e-09],\n",
       "         [2.2954e-07, 2.3288e-09, 2.4666e-09, 3.5603e-04, 3.7209e-10, 6.0136e-05,\n",
       "          3.5556e-04, 8.6100e-03, 1.0385e-10, 6.6911e-06, 8.9132e-07, 1.3650e-15,\n",
       "          2.8066e-02, 3.6939e-06, 7.0524e-07, 3.0179e-16, 2.9954e-12, 1.9045e-05,\n",
       "          6.8319e-07, 3.9826e-01, 5.7023e-19, 1.3491e-02, 4.5589e-08, 5.5076e-01,\n",
       "          8.9111e-09, 5.8631e-07, 9.0464e-06],\n",
       "         [1.4259e-07, 3.0682e-08, 5.4482e-06, 1.6365e-03, 3.0199e-08, 1.0394e-05,\n",
       "          9.8890e-04, 9.8575e-01, 3.4723e-10, 2.0909e-09, 8.5898e-12, 2.0161e-12,\n",
       "          9.7082e-05, 8.2979e-09, 2.8956e-12, 8.9804e-21, 6.5614e-12, 5.1191e-08,\n",
       "          5.0893e-10, 6.3321e-05, 6.0029e-19, 1.5183e-07, 2.9356e-08, 9.9184e-03,\n",
       "          1.3197e-14, 1.4949e-03, 3.5602e-05],\n",
       "         [2.7855e-14, 1.6985e-06, 6.0627e-18, 8.3190e-08, 5.5780e-11, 1.2369e-03,\n",
       "          3.8077e-14, 7.8435e-09, 8.9661e-14, 7.8518e-05, 1.3327e-04, 2.8368e-19,\n",
       "          4.9013e-05, 3.3382e-06, 9.9106e-01, 7.6785e-12, 3.7610e-18, 2.2260e-05,\n",
       "          1.1467e-10, 7.4168e-03, 1.5042e-14, 5.0019e-07, 4.1371e-13, 8.3605e-12,\n",
       "          4.4483e-12, 3.4461e-07, 1.6221e-06],\n",
       "         [5.8229e-10, 5.5302e-11, 3.7041e-01, 2.6504e-07, 2.5011e-13, 4.4135e-06,\n",
       "          5.9834e-06, 1.5190e-03, 6.2312e-09, 1.9385e-08, 1.6393e-08, 2.0993e-06,\n",
       "          1.7772e-03, 1.6263e-15, 9.4690e-13, 4.4926e-17, 1.5654e-06, 1.4587e-07,\n",
       "          2.0382e-04, 1.1506e-09, 2.3117e-09, 2.9050e-09, 5.0346e-01, 5.8451e-02,\n",
       "          2.1451e-15, 6.4172e-02, 6.7381e-09],\n",
       "         [2.2954e-07, 2.3288e-09, 2.4666e-09, 3.5603e-04, 3.7209e-10, 6.0136e-05,\n",
       "          3.5556e-04, 8.6100e-03, 1.0385e-10, 6.6911e-06, 8.9132e-07, 1.3650e-15,\n",
       "          2.8066e-02, 3.6939e-06, 7.0524e-07, 3.0179e-16, 2.9954e-12, 1.9045e-05,\n",
       "          6.8319e-07, 3.9826e-01, 5.7023e-19, 1.3491e-02, 4.5589e-08, 5.5076e-01,\n",
       "          8.9111e-09, 5.8631e-07, 9.0464e-06],\n",
       "         [1.2128e-10, 4.3400e-09, 5.9213e-17, 3.1332e-04, 4.1481e-12, 1.6646e-04,\n",
       "          9.7949e-08, 1.7756e-11, 9.5064e-10, 5.9435e-01, 6.1246e-03, 1.1159e-16,\n",
       "          9.4339e-03, 5.7627e-03, 2.3095e-01, 1.6200e-10, 1.4194e-13, 2.9596e-02,\n",
       "          1.7274e-03, 2.3541e-04, 4.1055e-15, 1.1960e-01, 1.6242e-08, 1.1113e-05,\n",
       "          1.7230e-03, 6.1453e-09, 1.1026e-05],\n",
       "         [1.4739e-02, 2.2063e-04, 2.4969e-06, 9.7275e-10, 8.2101e-10, 1.1713e-02,\n",
       "          1.8802e-02, 2.9682e-05, 1.8005e-07, 9.9594e-07, 1.4830e-09, 4.0567e-06,\n",
       "          4.2299e-06, 4.3175e-08, 3.1246e-09, 1.3859e-15, 1.0823e-12, 9.2722e-01,\n",
       "          1.2049e-08, 2.6028e-09, 1.4737e-05, 3.3606e-09, 9.7355e-06, 1.1878e-02,\n",
       "          1.2197e-13, 1.5363e-02, 3.0658e-08],\n",
       "         [1.8735e-03, 6.2878e-07, 1.2019e-01, 5.9163e-03, 5.1812e-06, 8.3700e-06,\n",
       "          7.2427e-01, 3.4639e-06, 1.4634e-01, 9.6471e-11, 6.5287e-13, 1.9159e-05,\n",
       "          4.8820e-10, 5.3285e-06, 1.0491e-13, 3.3961e-19, 2.0395e-11, 2.5155e-06,\n",
       "          8.6167e-15, 1.7831e-15, 3.4851e-17, 9.2467e-10, 5.0858e-06, 1.2823e-12,\n",
       "          1.3746e-13, 4.9660e-10, 1.3609e-03],\n",
       "         [6.3873e-12, 1.3787e-07, 3.3342e-15, 4.1413e-05, 6.7581e-08, 7.2312e-04,\n",
       "          2.6879e-14, 6.2707e-09, 2.4369e-11, 5.1793e-04, 1.2911e-04, 9.1411e-20,\n",
       "          2.1527e-01, 1.1849e-06, 7.3933e-01, 4.0488e-08, 4.1252e-13, 5.6261e-07,\n",
       "          3.3780e-06, 4.3945e-02, 9.8255e-17, 1.8934e-06, 1.2331e-10, 1.5231e-11,\n",
       "          3.6777e-09, 7.9203e-10, 3.4929e-05],\n",
       "         [6.0302e-09, 2.2317e-08, 1.1863e-07, 3.7260e-11, 9.5785e-16, 1.9316e-07,\n",
       "          1.3578e-08, 3.3450e-06, 4.8343e-09, 2.4296e-08, 3.2520e-08, 7.7127e-07,\n",
       "          1.1051e-08, 1.5813e-16, 3.3607e-11, 5.1082e-16, 2.2217e-12, 3.3285e-06,\n",
       "          7.4870e-10, 9.6747e-11, 8.9214e-06, 1.4574e-11, 5.2915e-08, 2.5761e-07,\n",
       "          1.4924e-19, 9.9998e-01, 2.6840e-12],\n",
       "         [6.2707e-09, 3.4216e-04, 1.0065e-01, 6.1375e-03, 1.1231e-05, 1.2517e-04,\n",
       "          1.0138e-08, 9.4900e-10, 8.1439e-01, 5.2535e-06, 4.2402e-03, 6.3288e-05,\n",
       "          1.1883e-06, 4.9066e-04, 1.1885e-06, 6.3547e-04, 2.2629e-04, 5.2686e-02,\n",
       "          1.6517e-11, 5.3178e-09, 1.9799e-10, 1.6005e-05, 1.5059e-04, 1.6286e-18,\n",
       "          4.8816e-08, 4.8966e-06, 1.9823e-02],\n",
       "         [1.4219e-09, 6.5527e-06, 8.2383e-01, 6.4251e-08, 2.4142e-09, 1.8408e-03,\n",
       "          1.1188e-10, 1.6120e-08, 7.4602e-04, 1.3843e-05, 3.1484e-02, 2.3853e-06,\n",
       "          3.3930e-05, 2.6834e-09, 5.0176e-09, 2.0755e-07, 8.5156e-02, 3.6869e-06,\n",
       "          4.9689e-07, 5.6854e-06, 9.4472e-09, 4.2834e-07, 6.6682e-03, 1.4592e-09,\n",
       "          2.2458e-11, 5.0196e-02, 9.1566e-06],\n",
       "         [1.2060e-09, 5.0424e-09, 1.0000e+00, 1.7247e-06, 4.1583e-13, 1.0023e-11,\n",
       "          2.4079e-09, 6.6137e-08, 2.3891e-06, 6.2831e-12, 2.2632e-16, 2.4753e-07,\n",
       "          1.1104e-08, 2.6994e-14, 1.3230e-20, 5.3071e-19, 4.0463e-08, 1.1386e-09,\n",
       "          6.1912e-11, 1.9433e-14, 2.4160e-19, 1.3309e-13, 2.4736e-07, 2.2085e-11,\n",
       "          3.9625e-16, 1.6393e-09, 1.9306e-07],\n",
       "         [2.2954e-07, 2.3288e-09, 2.4666e-09, 3.5603e-04, 3.7209e-10, 6.0136e-05,\n",
       "          3.5556e-04, 8.6100e-03, 1.0385e-10, 6.6911e-06, 8.9132e-07, 1.3650e-15,\n",
       "          2.8066e-02, 3.6939e-06, 7.0524e-07, 3.0179e-16, 2.9954e-12, 1.9045e-05,\n",
       "          6.8319e-07, 3.9826e-01, 5.7023e-19, 1.3491e-02, 4.5589e-08, 5.5076e-01,\n",
       "          8.9111e-09, 5.8631e-07, 9.0464e-06],\n",
       "         [1.8399e-07, 1.5418e-05, 1.2413e-11, 5.4612e-05, 2.6132e-10, 6.4425e-02,\n",
       "          4.9507e-07, 4.7600e-04, 1.5277e-10, 8.2298e-04, 3.9070e-05, 5.4322e-15,\n",
       "          6.5801e-03, 6.4490e-04, 2.6791e-03, 1.5822e-13, 7.5343e-16, 2.5025e-03,\n",
       "          7.1255e-08, 9.1177e-01, 5.8113e-15, 9.0996e-03, 5.2637e-09, 8.7253e-04,\n",
       "          7.0937e-09, 6.3931e-06, 1.3273e-05],\n",
       "         [9.2068e-12, 3.4364e-08, 7.7820e-16, 3.8646e-09, 3.9288e-16, 4.5945e-07,\n",
       "          1.3352e-09, 5.4170e-15, 4.4529e-12, 6.4561e-02, 9.5279e-03, 4.0637e-13,\n",
       "          9.9018e-05, 3.1164e-07, 1.6824e-03, 3.6457e-07, 3.3460e-13, 9.2269e-01,\n",
       "          1.4240e-03, 1.2072e-07, 8.9192e-12, 3.8763e-06, 3.3775e-08, 1.2591e-08,\n",
       "          8.6924e-06, 6.6673e-07, 5.2998e-08],\n",
       "         [9.7533e-02, 3.1195e-08, 2.0310e-01, 1.4837e-12, 1.7009e-08, 7.3317e-07,\n",
       "          6.4116e-02, 1.6119e-06, 1.4730e-02, 6.0293e-07, 4.0112e-10, 3.3989e-01,\n",
       "          2.1267e-10, 8.2744e-10, 5.2338e-14, 1.2440e-09, 1.1319e-07, 2.9036e-02,\n",
       "          7.0211e-09, 5.8928e-10, 3.5584e-04, 4.9014e-10, 2.5124e-01, 3.8281e-06,\n",
       "          3.4253e-09, 6.7148e-07, 2.3750e-08],\n",
       "         [1.1078e-09, 6.1125e-10, 1.3151e-13, 3.0555e-08, 2.3620e-09, 6.8047e-08,\n",
       "          8.3527e-10, 5.6098e-18, 8.0320e-05, 1.2238e-09, 2.0730e-12, 8.7676e-11,\n",
       "          4.9763e-11, 1.6431e-05, 2.3334e-13, 6.3055e-05, 1.8627e-15, 8.3671e-01,\n",
       "          3.7995e-14, 3.1012e-17, 5.3985e-17, 6.9713e-13, 1.0732e-10, 5.1122e-18,\n",
       "          1.6313e-01, 8.7318e-17, 2.0694e-09],\n",
       "         [1.5560e-06, 3.4914e-10, 2.0559e-12, 1.7475e-07, 8.4985e-11, 2.3058e-08,\n",
       "          7.6281e-10, 1.4800e-17, 1.0996e-06, 1.2609e-06, 1.3919e-08, 1.4646e-08,\n",
       "          3.7710e-07, 1.5251e-06, 2.3077e-12, 9.7934e-06, 4.0882e-14, 1.2134e-01,\n",
       "          2.6514e-09, 1.5184e-11, 2.2797e-14, 1.2900e-07, 4.0084e-10, 1.0421e-10,\n",
       "          8.7865e-01, 2.2210e-15, 6.2500e-09],\n",
       "         [1.4951e-03, 2.2380e-09, 7.4941e-01, 7.9988e-07, 2.5212e-05, 8.6192e-07,\n",
       "          2.4634e-01, 8.6050e-04, 1.4237e-03, 3.1268e-12, 2.5007e-17, 1.2776e-06,\n",
       "          2.1817e-13, 1.2999e-08, 1.0666e-17, 6.6286e-20, 1.3609e-10, 6.7930e-06,\n",
       "          7.0098e-15, 2.4277e-13, 3.6358e-11, 2.8782e-10, 3.6012e-07, 1.9272e-06,\n",
       "          2.0843e-12, 3.4469e-09, 4.3239e-04]]),\n",
       " torch.Size([32, 27]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob, prob.shape, prob[0].sum()\n",
    "# all the probabilities add to 1. We have just completed softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3f66a6bc-70e9-4e5e-92bf-3ce3f839ad61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we'll see how correctly \"prob\" predicts the next character for a given example out of the 32 examples.\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cf2e12fa-ccc0-4a98-8c54-a0832f7265e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll create an iterator over these values:\n",
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "10c1383f-5983-408d-bdb0-2c94d9846527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.0136e-05, 6.3692e-04, 2.1449e-12, 3.1427e-13, 1.8219e-05, 3.0179e-16,\n",
       "        6.2462e-04, 3.9474e-05, 1.7959e-12, 3.0083e-05, 3.1386e-08, 4.0785e-06,\n",
       "        2.3288e-09, 2.9356e-08, 1.6985e-06, 5.8229e-10, 6.6911e-06, 2.3541e-04,\n",
       "        2.2063e-04, 1.2019e-01, 7.2312e-04, 1.1051e-08, 1.1883e-06, 6.5527e-06,\n",
       "        1.2060e-09, 3.9826e-01, 1.5822e-13, 3.3460e-13, 1.4730e-02, 1.2238e-09,\n",
       "        3.4914e-10, 1.4951e-03])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we'll index over prob with this and Y to see how confident our model was for a given correct output.\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "40809e25-d056-4ffb-b1e8-487c8bae7308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.3260)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so ideally we want all of these to be 1. Right now these are all ready bad because we randomly assigned initially our weights and biases, i.e. the model is untrained.\n",
    "# Now, we can actually take the log of all of these, so that values closer to 0 are negative, and values closer to 1 are 0, so respectively punishing and rewarding the model.\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss # this is our unified, single value to show how good or bad the model has done with the given parameters. Right now its terrible with a loss of 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fa7f3521-a763-4010-9dda-3f09adc79f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, it's time we clean stuff up and make things a bit more organized from here on out.\n",
    "# ========================= Clean ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "09eebf2c-7f62-49b6-b664-225ea42b6a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ef9c3ce0-80c9-4398-a13f-2a37ce33f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d3df998b-fe3e-469f-ae56-fb7dffe004d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "06c346d3-5adf-4940-b674-5c978d806159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f6cddcda-7f97-4b91-93b2-1e904f363224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 17.76971435546875\n",
      "Iteration 2: 13.656402587890625\n",
      "Iteration 3: 11.298770904541016\n",
      "Iteration 4: 9.4524564743042\n",
      "Iteration 5: 7.984263896942139\n",
      "Iteration 6: 6.891321659088135\n",
      "Iteration 7: 6.1000142097473145\n",
      "Iteration 8: 5.452035903930664\n",
      "Iteration 9: 4.898152828216553\n",
      "Iteration 10: 4.414663791656494\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdims=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    loss = F.cross_entropy(logits, Y) # as it turns out, F.cross_entropy does precisely the above for us and is more efficient for several reasons, i think one of them about how it is stored in memory\n",
    "\n",
    "    print(f\"Iteration {i+1}: {loss.item()}\") # print the loss\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "678b12ae-621f-4962-8fbb-6b28839dd1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([10.7865, 12.2558, 17.3982, 13.2739, 10.6965, 10.7865,  9.5145,  9.0495,\n",
       "        14.0280, 11.8378,  9.9038, 15.4187, 10.7865, 10.1476,  9.8372, 11.7660,\n",
       "        10.7865, 10.0029,  9.2940,  9.6824, 11.4241,  9.4885,  8.1164,  9.5176,\n",
       "        12.6383, 10.7865, 10.6021, 11.0822,  6.3617, 17.3157, 12.4544,  8.1669],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 1,  8,  9,  0, 15,  1, 17,  2,  9,  9,  2,  0,  1, 15,  1,  0,  1, 19,\n",
       "         1,  1, 16, 10, 26,  9,  0,  1, 15, 16,  3,  9, 19,  1]))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "471c062e-87d5-4acb-9d50-ede0095c9bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5c3cc4be-b07e-4128-a0c4-b49a0d4a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, our model is predicting very very well, essentially overfitting, these few examples' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "85626d32-a70a-4262-a6f8-af6cb8d56522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe reason as to why our model\\'s loss is able to decrease so quickly is simply because\\nit is overfitting like crazy. First of all, we haven\\'t even done any splits in our\\ndataset to train, validate, and test. We can\\'t validate hyperparameters right now.\\nSecond, We have way too many parameters for the amount of predictions we\\'re trying\\nto make, i.e. for 5 words only, i.e. for only about 32 examples.\\nSo, we change all the way above our consideration from \"for w in words[:5]\"\\nto \"for w in words\".\\n'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The reason as to why our model's loss is able to decrease so quickly is simply because\n",
    "it is overfitting like crazy. First of all, we haven't even done any splits in our\n",
    "dataset to train, validate, and test. We can't validate hyperparameters right now.\n",
    "Second, We have way too many parameters for the amount of predictions we're trying\n",
    "to make, i.e. for 5 words only, i.e. for only about 32 examples.\n",
    "So, we change all the way above our consideration from \"for w in words[:5]\"\n",
    "to \"for w in words\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd77f2-55aa-4f99-8bcf-4b7acc64a47a",
   "metadata": {},
   "source": [
    "# We will go from working with only the first five words to working with the entire dataset in a seperate .ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69a34c-a33d-4dce-bd95-a8c729ebf1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
