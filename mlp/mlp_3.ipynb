{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffda168-e112-4145-983d-b0ee75ef7823",
   "metadata": {},
   "source": [
    "# This notebook continues from `mlp_2.ipynb`, where we start splitting our dataset into train/dev/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a3382a3-7692-4495-a34e-dfa0c242c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd2e07b1-4f12-48c0-87d1-21958595a7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd5b7235-6509-4577-baa3-791320c33d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocab of chars and mappings to/from ints in alphabetical order\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bda68e86-5866-4f61-83d0-b00196fa8699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182424, 3]) torch.Size([182424])\n",
      "torch.Size([22836, 3]) torch.Size([22836])\n",
      "torch.Size([22886, 3]) torch.Size([22886])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "192109d7-ee4e-4094-9d1f-29697413790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator=g)\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5edbc390-e5c2-497f-9415-9c1711d2cdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "44cbb06c-0578-4d94-b860-532b2e62269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "efa91336-34f1-41a9-8fd1-2a181055e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3903088569641113\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    # minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e346940c-0737-47f4-aaf9-c75c25c1a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.564584255218506\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac875e8-3620-4bfe-9c2a-eb9fab054c3c",
   "metadata": {},
   "source": [
    "## Validation\n",
    "A quick check above shows that the validation loss is roughly equal to this training loss we're getting. What that means is that our model is not powerful enough to be merely memorizing the data as before, but rather it is **underfitting** and that means that it is very small and that we should expect to make performance improvements by **scaling the neural network**.\n",
    "\n",
    "We will perform two experiments to try to scale it: \n",
    "1. Larger hidden layer\n",
    "2. Larger embedding size (i.e. increase m in R^m)\n",
    "\n",
    "We will do this in one last notebook, namely `mlp_4.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
